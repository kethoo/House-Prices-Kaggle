# House Prices - Advanced Regression Techniques

Competition Link -> https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques

კონკურსის ფარგლებში ჩვენი მთავარი მიზანი იყო მოცემულ მონაცემებზე დაყრდნობით, განგვესაზღვრა სახლის ფასები.  
დაწყებული მიმოხილვით, თუ როგორ არის თითოეული მახასიათებელი დამოკიდებული ფასზე, მისი ზრდა ან შემცირება, ასევე მათი ურთიერთდაკავშირების გაანალიზებით.  
შემდგომ, დავამუშავე მონაცემები კატეგორიულ და რიცხვობრივ ნაწილებად, კატეგორიული მონაცემები კიდევ ორდინალურ და ნომინალურ ნაწილებად გავყავი და განსხვავებულად დამუშავე.  

მონაცემებიდან გავამ筛არე ის მახასიათებლები, რომლებსაც გავლენა არ ჰქონდათ ფასზე, ბევრ მაჩვენებელს ვაკლდი Missing Values-ის გამო ან მაღალი კორელაცია ჰქონდათ სხვა მახასიათებლებთან.  
შექმენი რამდენიმე ახალი მახასიათებელი და დავწმინდე დამხმარე მახასიათებლები.

---

## 🔹 რეპოზიტორის სტრუქტურა

- `model_experiment.ipynb` — მთავარი ნაწილი, სადაც ხდება მონაცემების დამუშავება: Cleaning, Feature Engineering, Feature Selection და Training.  
- `model_inference.ipynb` — ტესტ სეტზე პროგნოზის გენერაცია XGBoost მოდელით და submissions-ის შექმნა.  
- `README.md` — მუშაობის პროცესი, გამოყენებული მიდგომები და შედეგები.

---

## 🔹 Feature Engineering

- მონაცემები დავყავი კატეგორიულ და რიცხვობრივ ნაწილებად.  
- კატეგორიული დავყავი ორდინალურ (Ordinal) და ნომინალურ (Nominal) ტიპებად, შესაბამისად გამოვიყენე Woe encoding და One Hot encoding.  
- Nan მნიშვნელობებისთვის კატეგორიულ სვეტებში ჩავანაცვლე მოდალური მნიშვნელობით, რიცხვითებში — საშუალო მნიშვნელობით.  
- მოვიშორე ძალიან ბევრი Missing Values და მაღალი კორელაციის მქონე მახასიათებლები (multicollinearity-ის თავიდან ასაცილებლად).  
- შევქმენი ახალი მახასიათებლები და ნაწილობრივ დავხურე ის, რომლებიც მათ შესაქმნელად გამოიყენე.

---

## 🔹 Feature Selection

- 3 მახასიათებლის 93%-ზე მეტი და 1-ის 80% Missing Values-ის გამო ისინი მოშორებული იქნა.  
- მაღალი კორელაციის მქონე მახასიათებლები გამორიცხული იყო Pearson კორელაციის მიხედვით.  
- Feature Importance შეფასდა Random Forest და XGBoost მოდელების საშუალებით.  
- Lasso რეგულარიზაციით (L1) არარელევანტური მახასიათებლების წონა ნულამდე დაიყვანეს.

---

## 🔹 Training

- 4 რეგრესიის მოდელი:  
  - Linear Regression  
  - Lasso Regression (L1 regularization)  
  - Random Forest Regressor  
  - XGBoost Regressor  

- პარამეტრების ოპტიმიზაცია განხორციელდა GridSearchCV-თ (Lasso, Random Forest, XGBoost).  
- Cross-validation (cv=3 ან cv=5) გამოიყენება შეფასებისა და overfitting-ის თავიდან ასაცილებლად.  

- საბოლოოდ შეფასდა მოდელები შემდეგი მეტრიკებით:  
  - R² და Adjusted R²  
  - RMSE და MAE  
  - RMSLE  
  - F-statistic  
  - CV_RMSE  

- საუკეთესო მოდელი გამოვლინდა XGBoost, დაბალი CV_RMSE და მაღალი R²/Adjusted R²-ის საფუძველზე.

---

## 🔹 MLflow Tracking

- ექსპერიმენტების ბმული:  
  [MLflow Experiment](https://dagshub.com/kechik21/ML_HW1.mlflow/#/experiments/0)

---

### მეტრიკების აღწერა

| მეტრიკა      | აღწერა                                        |
|-------------|----------------------------------------------|
| R2          | კორელაციის ხარისხი (0–1), უფრო მაღალი უკეთესია |
| Adjusted R2 | R²-ის გაწონასწორებული ვერსია, პარამეტრების რაოდენობას ითვალისწინებს |
| RMSE        | Root Mean Squared Error — საშუალო შეცდომის კვადრატული ფესვი |
| MAE         | Mean Absolute Error — საშუალო აბსოლუტური შეცდომა  |
| RMSLE       | Root Mean Squared Log Error — ზრდის სიჩქარის შესაფასებლად  |
| F-statistic | მოდელის სტატისტიკური მნიშვნელობის ტესტი       |
| CV_RMSE     | Cross-validated RMSE — მოდელის გენერალიზაციის მაჩვენებელი |

---

### საუკეთესო მოდელის შედეგები: XGBoost

| Metric       | Value              |
|--------------|--------------------|
| R2           | 0.9228             |
| MAE          | 13,330.90          |
| Adjusted R2  | 0.9038             |
| RMSE         | 19,481.43          |
| F-statistic  | 50.19              |
| CV_RMSE      | 22,838.35          |
| RMSLE        | 0.1123             |

---

**Final Score:** 0.13273
